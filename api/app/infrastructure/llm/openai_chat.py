"""
OpenAI chat service.
"""
from typing import List, Optional
from openai import AsyncOpenAI
import logging

from app.domain.ports.llm_service import LLMServicePort
from app.core.config import settings

logger = logging.getLogger(__name__)


class OpenAIChatService(LLMServicePort):
    """
    Service for generating chat responses using OpenAI.
    """

    def __init__(self):
        self.api_key = settings.OPENAI_API_KEY
        self.model = "gpt-5"  # Fast and cost-effective
        # Create client once at initialization to avoid httpx wrapper issues
        self._client = AsyncOpenAI(api_key=self.api_key) if self.api_key else None

    def _get_client(self) -> AsyncOpenAI:
        """Get OpenAI client."""
        if not self._client:
            raise ValueError("OPENAI_API_KEY not configured in environment variables")
        return self._client

    async def generate_response(
        self,
        query: str,
        context: List[str],
        system_prompt: Optional[str] = None
    ) -> str:
        """
        Generate a response based on query and context.

        Args:
            query: User question
            context: List of relevant text chunks (documents or conversation history)
            system_prompt: Optional system prompt to guide the model

        Returns:
            Generated response text
        """
        client = self._get_client()

        # Default system prompt for financial assistant
        if not system_prompt:
            system_prompt = """Eres un asistente financiero inteligente y amigable.

Tu tarea es ayudar al usuario respondiendo sus preguntas de manera profesional y √∫til.

Reglas importantes:
- Si el contexto incluye documentos financieros, √∫salos para responder con datos precisos
- Si el contexto incluye historial de conversaci√≥n, √∫salo para mantener coherencia
- Si no hay suficiente informaci√≥n en el contexto para responder una pregunta espec√≠fica sobre documentos, indica claramente que no tienes esa informaci√≥n
- Puedes saludar cordialmente y mantener conversaciones casuales
- S√© amable, preciso y conciso
- Responde siempre en espa√±ol"""

        # Build context text
        if not context:
            # No context available - conversational mode
            context_text = "No hay documentos cargados a√∫n. Puedes mantener una conversaci√≥n general."
        else:
            context_text = "\n\n---\n\n".join(context)

        # Build messages
        messages = [
            {"role": "system", "content": system_prompt},
            {
                "role": "user",
                "content": f"""Contexto disponible:

{context_text}

---

Pregunta del usuario: {query}

Por favor, responde de manera √∫til bas√°ndote en el contexto si est√° disponible."""
            }
        ]

        # Log LLM call
        logger.info(f"ü§ñ Calling LLM - Model: {self.model} | Query: '{query[:50]}...' | Context chunks: {len(context)}")

        # Generate response
        try:
            response = await client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_completion_tokens=1000
            )

            content = response.choices[0].message.content

            if not content:
                logger.warning(f"Empty response from LLM for query: '{query}'")
                return "Lo siento, no pude generar una respuesta en este momento. Por favor, intenta reformular tu pregunta."

            return content.strip()

        except Exception as e:
            logger.error(f"Error calling OpenAI API: {str(e)}", exc_info=True)
            raise

    async def close(self):
        """Close the OpenAI client and cleanup resources."""
        if self._client:
            try:
                # OpenAI wraps httpx client, which can cause AttributeError on close
                # We catch and ignore it as the client will be garbage collected
                await self._client.close()
            except AttributeError:
                pass  # Ignore the _state error from AsyncHttpxClientWrapper
            finally:
                self._client = None